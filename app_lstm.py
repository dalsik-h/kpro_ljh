import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
# from keras.models import load_model
from tensorflow.keras.models import load_model
from scipy.signal import savgol_filter
from datetime import timedelta

# =============================
# Streamlit UI
# =============================
st.header("ğŸ“ˆ ëŒ ìˆ˜ìœ„ ì˜ˆì¸¡ (LSTM+GRU ëª¨ë¸)")

uploaded_future = st.file_uploader(" â¡ï¸ ì˜ˆì¸¡ê¸°ê°„ì— ëŒ€í•œ ë…ë¦½ë³€ìˆ˜ ë°ì´í„° (íŒŒì¼ëª…: future_input2.csv)", type=["csv"])
uploaded_history = st.file_uploader(" â¡ï¸ ê³¼ê±°(4ì¼) ì „ì²´(ë…ë¦½+ì¢…ì†)ë³€ìˆ˜ ë°ì´í„° (íŒŒì¼ëª…: last_168_input.csv)", type=["csv"])

if uploaded_future and uploaded_history:
    # =============================
    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    # =============================
    raw_future = pd.read_csv(uploaded_future, parse_dates=['date_time'], index_col='date_time')
    history_df = pd.read_csv(uploaded_history, parse_dates=['date_time'], index_col='date_time')
    st.success("CSV íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")

    raw_future.index = pd.to_datetime(raw_future.index)
    history_df.index = pd.to_datetime(history_df.index)
    raw_future.sort_index(inplace=True)
    history_df.sort_index(inplace=True)

    # =============================
    # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¶ˆëŸ¬ì˜¤ê¸°
    # =============================
    # model = load_model("recursive_lstm_model.keras", compile=False)
    model = load_model("recursive_lstm_model.h5")
    model.compile(optimizer='adam', loss='mae')
    scaler_input = joblib.load('scaler_input.pkl')
    scaler_target = joblib.load('scaler_target.pkl')

    # =============================
    # ì „ì²˜ë¦¬ í•¨ìˆ˜
    # =============================
    def prepare_initial_input(future_df, is_future=True):
        df = future_df.copy()
        if is_future:
            df['ycd_level'] = np.nan
        df['month'] = df.index.month
        df['season'] = df['month'] % 12 // 3 + 1
        season_onehot = pd.get_dummies(df['season'], prefix='season').astype(int)
        df = pd.concat([df, season_onehot], axis=1)
        for s in ['season_1', 'season_2', 'season_3', 'season_4']:
            if s not in df.columns:
                df[s] = 0
        df.drop(['season', 'month'], axis=1, inplace=True)
        df['ycd_gflow_out1'] = np.log1p(df['ycd_gflow_out1'])
        df['over_flow_out5'] = np.log1p(df['over_flow_out5'])
        for lag in [1, 2, 3, 4, 5, 6, 24]:
            df[f'ycd_level_lag{lag}'] = np.nan
        df['flow_out1_rolling_12h'] = np.nan
        df['flow_out1_rolling_12h_diff'] = np.nan
        df['rain_count_12h'] = np.nan
        return df

    # =============================
    # ì¬ê·€ ì˜ˆì¸¡ í•¨ìˆ˜
    # =============================
    def recursive_forecast(future_df, history_df, model, scaler_input, scaler_target, lookback=96):
        df = prepare_initial_input(future_df, is_future=True)
        df = df[scaler_input.feature_names_in_]
        hdf = prepare_initial_input(history_df, is_future=False)
        for t in range(24, len(hdf)):
            current_time = hdf.index[t]

            for lag in [1, 2, 3, 4, 5, 6, 24]:
                lag_time = current_time - pd.Timedelta(hours=lag)
                if lag_time in hdf.index:
                    val = hdf.at[lag_time, 'ycd_level']
                    hdf.at[current_time, f'ycd_level_lag{lag}'] = val

            past_flow = hdf.loc[current_time - pd.Timedelta(hours=11):current_time, 'ycd_gflow_out1']
            past_flow2 = hdf.loc[current_time - pd.Timedelta(hours=12):current_time - pd.Timedelta(hours=1), 'ycd_gflow_out1']
            if len(past_flow) == 12:
                mean1 = past_flow.mean()
                mean2 = past_flow2.mean()
                hdf.at[current_time, 'flow_out1_rolling_12h'] = mean1
                hdf.at[current_time, 'flow_out1_rolling_12h_diff'] = mean1 - mean2

            past_rain = hdf.loc[current_time - pd.Timedelta(hours=11):current_time, 'rain_total']
            if len(past_rain) == 12:
                hdf.at[current_time, 'rain_count_12h'] = (past_rain > 0).sum()

        full_data = pd.concat([hdf, df], axis=0)
        preds = []
        forecast_index = []

        bias = 0  # ì´ˆê¸°í™”
        initial_slope = 0
        initial_level = history_df['ycd_level'].iloc[-1]  # ë§ˆì§€ë§‰ ì‹¤ì œê°’

        for t in range(len(df)):
            current_time = df.index[t]

            for lag in [1,2,3,4,5,6,24]:
                lag_time = current_time - pd.Timedelta(hours=lag)
                if lag_time in full_data.index:
                    val = full_data.at[lag_time, 'ycd_level']
                    df.at[current_time, f'ycd_level_lag{lag}'] = val
                    full_data.at[current_time, f'ycd_level_lag{lag}'] = val
            past_flow = full_data.loc[current_time - pd.Timedelta(hours=11):current_time, 'ycd_gflow_out1']
            past_flow2 = full_data.loc[current_time - pd.Timedelta(hours=12):current_time - pd.Timedelta(hours=1), 'ycd_gflow_out1']
            if len(past_flow) == 12:
                df.at[current_time, 'flow_out1_rolling_12h'] = past_flow.mean()
                df.at[current_time, 'flow_out1_rolling_12h_diff'] = past_flow.mean() - past_flow2.mean()
                full_data.at[current_time, 'flow_out1_rolling_12h'] = past_flow.mean()
                full_data.at[current_time, 'flow_out1_rolling_12h_diff'] = past_flow.mean() - past_flow2.mean()
            past_rain = full_data.loc[current_time - pd.Timedelta(hours=11):current_time, 'rain_total']
            if len(past_rain) == 12:
                df.at[current_time, 'rain_count_12h'] = (past_rain > 0).sum()
                full_data.at[current_time, 'rain_count_12h'] = (past_rain > 0).sum()
            input_row = df.loc[[current_time]].copy()
            input_row_scaled = pd.DataFrame(
                scaler_input.transform(input_row[scaler_input.feature_names_in_]),
                columns=scaler_input.feature_names_in_,
                index=input_row.index
            )
            seq_start_time = current_time - pd.Timedelta(hours=lookback)
            seq_end_time = current_time - pd.Timedelta(hours=1)
            sequence = full_data.loc[seq_start_time:seq_end_time, scaler_input.feature_names_in_]
            if len(sequence) < lookback:
                continue
            sequence_scaled = scaler_input.transform(sequence)
            sequence_scaled = sequence_scaled.reshape(1, lookback, -1)
            pred_scaled = model.predict(sequence_scaled, verbose=0)
            pred = scaler_target.inverse_transform(pred_scaled)[0][0]

            ##########################################################################################
            if t == 0:
                bias = initial_level - pred
                pred += bias
                initial_slope = 0
            else:
                pred += bias
                slope = pred - preds[-1]
                ratio = min(t / len(df), 1)
                if slope > 0:
                    # pred = preds[-1] + slope
                    if t < int(len(df) * 0.4):  # ì˜ˆ: 40% ì‹œì  ì´ì „ì—” ì™„ë§Œí™” ì—†ìŒ
                        pred = preds[-1] + slope  # slope ê·¸ëŒ€ë¡œ ë°˜ì˜
                    else:
                        weaken_ratio = np.exp(-7 * (ratio - 0.4) / 0.6)
                        pred = preds[-1] + slope * weaken_ratio
                    initial_slope = slope
                else:
                    # âœ… í•˜ê°•ì´ë¼ë©´ ì™„ë§Œí•˜ê²Œ ì¡°ì • (í›„ë°˜ ê°ˆìˆ˜ë¡ ì™„ë§Œí™”)
                    # weaken_ratio = max(0.5, 1 - math.log1p(ratio * 9) / math.log1p(10))
                    weaken_ratio = max(0.3, np.exp(-2 * ratio))
                    pred = preds[-1] + slope * weaken_ratio
                    initial_slope = slope * weaken_ratio
            ##########################################################################################

            preds.append(pred)
            forecast_index.append(current_time)
            df.at[current_time, 'ycd_level'] = pred
            full_data.at[current_time, 'ycd_level'] = pred
        return forecast_index, preds

    # =============================
    # ì˜ˆì¸¡ ì‹¤í–‰
    # =============================
    forecast_index, forecast_preds = recursive_forecast(raw_future, history_df, model, scaler_input, scaler_target)

    # ê²°ê³¼ ì •ë¦¬ ë° ì‹œê°í™”
    forecast_df = pd.DataFrame({'date_time': forecast_index, 'predicted_ycd_level': forecast_preds})
    forecast_df['predicted_ycd_level'] = forecast_df['predicted_ycd_level'].round(2)
    forecast_df.set_index('date_time', inplace=True)

    st.success("ì˜ˆì¸¡ ì™„ë£Œ!")

        # ì˜ˆì¸¡ ê²°ê³¼ í‘œë¡œ ë¨¼ì € ì¶œë ¥
    st.subheader("ğŸ“‹ ì˜ì²œëŒ ìˆ˜ìœ„ ì˜ˆì¸¡ ê²°ê³¼ í‘œ")
    st.dataframe(forecast_df)  


    st.subheader("ğŸ“ˆ ì˜ì²œëŒ ìˆ˜ìœ„ ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë˜í”„")

    plt.figure(figsize=(12, 4))
    plt.plot(forecast_df.index, forecast_df['predicted_ycd_level'], label='Predicted Level')
    plt.xlabel('Date/Time')
    plt.ylabel('Dam Level')
    plt.title('Predicted Dam Water Level')

    # ğŸ”½ yì¶• ìë™ ì¡°ì •
    ymin = forecast_df['predicted_ycd_level'].min()
    ymax = forecast_df['predicted_ycd_level'].max()
    padding = (ymax - ymin) * 0.1
    plt.ylim(ymin - padding, ymax + padding)

    plt.grid(True)
    plt.legend()
    st.pyplot(plt)

    # =============================
    # ì•ˆê³„ì†Œìˆ˜ë ¥ ë°œì „ì „ë ¥ ì˜ˆì¸¡ ë¶€ë¶„
    # =============================
    st.header("ğŸ”‹ ì•ˆê³„ì†Œìˆ˜ë ¥ ë°œì „ì „ë ¥ ì˜ˆì¸¡ (XGBoost ëª¨ë¸)")

    hpower_future = st.file_uploader(" â¡ï¸ ì˜ˆì¸¡ê¸°ê°„ì— ëŒ€í•œ ë…ë¦½ë³€ìˆ˜ ë°ì´í„° (íŒŒì¼ëª…: future_input3.csv)", type=["csv"])


    if hpower_future is not None:

        hpower_df = pd.read_csv(hpower_future, parse_dates=['date_time'])
        hpower_df.sort_values('date_time', inplace=True)

        # ========================
        # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¶ˆëŸ¬ì˜¤ê¸°
        # ========================
        model2 = joblib.load('final_xgb_model.pkl')
        scaler2 = joblib.load('step3_standard_scaler.pkl')

        # ========================
        # 1ì°¨ë¡œ ì˜ˆì¸¡ëœ ëŒìˆ˜ìœ„ í™œìš©
        # ========================
        forecast_df = forecast_df.rename(columns={'predicted_ycd_level': 'ycd_level'})

        # ========================
        # ë°ì´í„° ë³‘í•© ë° ì •ë ¬
        # ========================
        merged_df = pd.merge(hpower_df, forecast_df, on='date_time', how='inner')

        # ì—´ ìˆœì„œ ë§ì¶”ê¸° (ëª¨ë¸ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìˆœì„œ)
        expected_features = ['agp_bp_vv', 'ycd_level', 'thj_vv', 'agp_inflow']
        model_input = merged_df[expected_features]

        # ========================
        # ìŠ¤ì¼€ì¼ë§ ë° ì˜ˆì¸¡
        # ========================
        scaled2_input = scaler2.transform(model_input)
        predictions = model2.predict(scaled2_input)
        merged_df['predicted_agp_power'] = np.floor(predictions).astype(int)
        # ========================
        # ê²°ê³¼ ì¶œë ¥
        # ========================
        st.success("ì˜ˆì¸¡ ì™„ë£Œ!")

        # ì˜ˆì¸¡ ê²°ê³¼ í‘œë¡œ ë¨¼ì € ì¶œë ¥
        st.subheader("ğŸ“‹ ì•ˆê³„(ì†Œ) ë°œì „ì „ë ¥ ì˜ˆì¸¡ ê²°ê³¼ í‘œ")
        st.dataframe(merged_df)

        # ========================
        # ì‹œê°í™”
        # ========================
        st.subheader("ğŸ“ˆ ì•ˆê³„(ì†Œ) ë°œì „ì „ë ¥ ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë˜í”„")

        plt.figure(figsize=(12, 5))
        plt.plot(merged_df['date_time'], merged_df['predicted_agp_power'], marker='o', linestyle='-', label='predicted agp_power')
        plt.xlabel('Date / Time')
        plt.ylabel('Predicted agp_power')
        plt.title('XGBoost agp_power Prediction')
        plt.grid(True)
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()

        st.pyplot(plt)


        # =========================================
        # ìµœì¢… ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„± (ëŒ ìˆ˜ìœ„ & ë°œì „ì „ë ¥)
        # =========================================

        def _dir_word(a, b):
            """aâ†’b ë³€í™” ë°©í–¥ì„ í•œêµ­ì–´ë¡œ ë°˜í™˜"""
            if b > a:
                return "ìƒìŠ¹"
            elif b < a:
                return "í•˜ê°•"
            else:
                return "ë³€í™” ì—†ìŒ"

        def _pick_points_from_index(idx_like):
            """DatetimeIndexë‚˜ datetime ì‹œë¦¬ì¦ˆì—ì„œ ì‹œì‘/ì¤‘ê°„/ë ì‹œì ê³¼ í¬ë§· ë¬¸ìì—´ ë°˜í™˜"""
            start_ts = idx_like[0]
            mid_ts   = idx_like[len(idx_like)//2]
            end_ts   = idx_like[-1]
            return (
                start_ts.strftime("%Y-%m-%d %H:%M"),
                mid_ts.strftime("%Y-%m-%d %H:%M"),
                end_ts.strftime("%Y-%m-%d %H:%M"),
                start_ts, mid_ts, end_ts
            )

        # ---- (1) ëŒ ìˆ˜ìœ„ ìš”ì•½ ----
        if not forecast_df.empty:
            # ì‹œì  ì„ íƒ
            s_str, m_str, e_str, s_ts, m_ts, e_ts = _pick_points_from_index(forecast_df.index)

            # ê°’ ì„ íƒ
            s_level = float(forecast_df.loc[s_ts, "predicted_ycd_level"])
            m_level = float(forecast_df.loc[m_ts, "predicted_ycd_level"])
            e_level = float(forecast_df.loc[e_ts, "predicted_ycd_level"])

            # ë°©í–¥ íŒë‹¨
            dir_level_1 = _dir_word(s_level, m_level)   # ì‹œì‘â†’ì¤‘ê°„
            dir_level_2 = _dir_word(m_level, e_level)   # ì¤‘ê°„â†’ì¢…ë£Œ

            # ---- (2) ë°œì „ì „ë ¥ ìš”ì•½ (ìˆì„ ë•Œë§Œ) ----
            has_power = "hpower_df" in locals() and "merged_df" in locals() and not merged_df.empty

            st.subheader("ğŸ“ ìµœì¢… ìš”ì•½")

            # ëŒ ìˆ˜ìœ„ ë¬¸ì¥
            st.markdown(
                f"""
        **ëŒ ìˆ˜ìœ„**ëŠ” **{s_str}** ê¸°ì¤€ **{s_level:.2f}**ìœ¼ë¡œ ì‹œì‘í•´ **ì¤‘ê°„ì‹œì ({m_str})**ì—ëŠ” **{m_level:.2f}**ê¹Œì§€ **{dir_level_1}**í•  ì˜ˆì •ì´ë©°,  
        **ì¢…ë£Œì‹œì ({e_str})**ì—ëŠ” **{e_level:.2f}**ë¡œ **{dir_level_2}**í•  ì˜ˆì •ì…ë‹ˆë‹¤.
                """
            )

            # ë°œì „ì „ë ¥ ë¬¸ì¥ (ë°œì „ì „ë ¥ ì˜ˆì¸¡ê¹Œì§€ ì™„ë£Œëœ ê²½ìš°)
            if has_power:
                # ì‹œê°„ëŒ€ ë™ì¼ ë²”ìœ„ ë³‘í•© ê²°ê³¼(merged_df)ì—ì„œ ì‹œì‘/ì¤‘ê°„/ì¢…ë£Œ ì‹œì 
                s2_str, m2_str, e2_str, s2_ts, m2_ts, e2_ts = _pick_points_from_index(merged_df["date_time"])

                # ê°’ ì¶”ì¶œ (ì •ìˆ˜)
                s_pow = int(merged_df.loc[merged_df["date_time"] == s2_ts, "predicted_agp_power"].iloc[0])
                m_pow = int(merged_df.loc[merged_df["date_time"] == m2_ts, "predicted_agp_power"].iloc[0])
                e_pow = int(merged_df.loc[merged_df["date_time"] == e2_ts, "predicted_agp_power"].iloc[0])

                # ë°©í–¥ íŒë‹¨
                dir_pow_1 = _dir_word(s_pow, m_pow)
                dir_pow_2 = _dir_word(m_pow, e_pow)

                st.markdown(
                    f"""
        ì´ì— ë”°ë¼ **ë°œì „ì „ë ¥**ì€ **{s2_str}** ê¸°ì¤€ **{s_pow}**ë¡œ ì‹œì‘í•´ **ì¤‘ê°„ì‹œì ({m2_str})**ì—ëŠ” **{m_pow}**ê¹Œì§€ **{dir_pow_1}**í•˜ë©°,  
        **ì¢…ë£Œì‹œì ({e2_str})**ì—ëŠ” **{e_pow}**ë¡œ **{dir_pow_2}**í•  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤.
                    """
                )